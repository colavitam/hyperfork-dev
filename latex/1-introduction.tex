% !TEX root = top.tex above command is so that compilation is always from
% top.tex
\section{Introduction} \label{sec:intro}

\Paragraph{Serverless Computing} Serverless computing, also known as
\emph{Function-as-a-Service} (FaaS), has become an increasingly prevalent
platform in the cloud computing ecosystem. In an attempt to realize a vision of
computation as a utility, serverless computing allows users to run application
code in response to triggers without provisioning infrastructure. In comparison
to current cloud computing platforms, clients no longer have to maintain
virtual machine images, are billed only for application computation performed
in response to requests, and benefit from autoscaling to handle variable
request rates. Platforms are available from all major cloud computing
companies, including Amazon Lambda, Azure Functions, and Google Cloud
Functions. Several significant enterprise customers have moved parts of their
services onto serverless platforms, including the news site The
Guardian.\footnote{https://aws.amazon.com/solutions/case-studies/the-guardian/}.

\Paragraph{Serverless Infrastructure} The typical implementation of a
serverless computing platform places user-submitted functions onto dynamically
created Virtual Machines (VMs). These functions are intended to be light-weight
stateless single-process programs. Because of the relatively short run-times of
serverless functions, a critical performance constraint in serverless
infrastructure implementations is the scheduling latency of functions--mainly
comprising of the creation time for instance VMs. A standard optimization
employed for this start-up time is to keep instance VMs running for a period of
time, and schedule function requests onto existing VMs. This leads to a number
of other properties, including limits on function run-time and the potential
for functions to be terminated at any time. This is necessary for the scheduler
to auto-scale instances efficiently. Another property is the clear split in
individual function start-up latencies between \emph{warm-starts}, where a function
is scheduled to an already running instance, and \emph{cold-starts}, where a new VM
must be created for the function. Functions from different users are usually
not placed in the same VM for security and isolation reasons, but one VM can
host several instances of one user's function.

\Paragraph{The Problem with Serverless: Cold-start} A central promise of
serverless computing services is rapid scalability. Meeting this demand at
scale requires that new function instances can be started very quickly to
service incoming requests. This is easy when there exist currently running
warm instances but more difficult when a new cold instance must be started.
Prevous work has recorded Amazon Lambda warmstart latencies of around 25ms, and
cold-start latencies of 250ms~\cite{peeking}. For cold-start, one significant
bottleneck is the creation of a new VM. In the Amazon Firecracker
specification~\cite{firecracker-spec}, new VMs are specified to boot in under
125ms, a significant chunk of the coldstart latency. Pools or buffers of ready
VMs may help, but to approach warmstart latencies, improvement in VM creation
latencies will be needed. To create a VM, the host Hypervisor/Virtual Machine
Monitor (VMM) must initialize virtual resources including CPUs, memory, and
other devices, then the guest kernel must be loaded from disk and initialized
in guest memory, then the guest kernel must be booted, and finally the function
runtime/process can be started and the request handled.

\Paragraph{Flash Cloning} Three significant parts of VM creation include (1)
loading the kernel into memory, (2) booting the guest kernel,
and (3) loading potentially large libraries/runtimes. Management operations
within the VMM such as virtual device creation and resource allocation are
relatively cheap compared to the start-up within a guest and the copying of
guest memory. Optimizing these steps could dramatically reduce the startup
latency of new serverless functions. One technique to do this is to employ
\emph{flash cloning}. Instead of loading VM images from disk and booting a
kernel, we propose cloning existing reference VMs in memory. Additionally, we
propose a copy-on-write mechanic to reduce both the copy time overhead and the
memory pressure of packing many VMs onto one host. This method can be compared
to the Unix fork abstraction.


\begin{quote} To create an isolated virtual machine, rather than re-create the
entire world, we should only re-create the necessarily distinct VMM components
and clone identical guest memory and execution context from ready-to-go
reference VMs.
\end{quote}

From this insight we present \emph{HyperFork}, a KVM-based VM cloning
implementation for the context of serverless computing.

\Paragraph{Contributions} Our contributions are as follows:
\begin{itemize}
\item HyperFork, A KVM-based virtual machine cloning implementation which
outperforms standard VM creation latencies by up to two orders of magnitude
\item A thorough analysis of the performance of Hyperfork for both VM creation
latency as well as performance degredation with respect to copy-on-write memory
sharing
\end{itemize}

Our implementation and benchmarks are open source and available at %TODO: link

The rest of the paper is organized as follows. Section~\ref{sec:related} provides a summary of related work. Section~\ref{sec:background} describes the technologies HyperFork is built upon. Section~\ref{sec:implementation} describes the design and architecture of HyperFork. We evaluate its performance in Section~\ref{sec:evaluation}, discuss the results in Section~\ref{sec:discussion}, and then outline current limitations and future directions in Section~\ref{sec:future}.
