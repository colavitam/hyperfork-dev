% !TEX root = top.tex above command is so that compilation is always from
% top.tex
\section{Experimental Evaluation} \label{sec:experiments} We now describe the
design of our benchmarks for the kvmtool HyperFork implementation. We have
conducted both microbenchmarks, to test the overhead of fork and copy-on-write
page duplication, and end-to-end benchmarks to measure throughput and resource
utilization improvements.

\Paragraph{Experimental Setup}
We perform all tests on an m5.metal instance from Amazon Web Services EC2,
which features 96 logical processors and 384 GB of memory. The file system
image used for all tests is a minimal Linux setup generated using the buildroot
utility.\footnote{buildroot.org} This specially generated image contains only
the minimal set of utilities and software needed for our benchmarks. In typical
use, the guest images used for FaaS platforms may be more fully featured and
optimized, but such single purpose images have no need for the flexibility of a
general purpose Linux distribution image. Since FaaS uses very short-lived
instances, there is no need for package managers, update utilities, or other
maintenance software.

Using a minimal image also presents cold-boot times in an idealistic
environment, as a heavier image would take longer to boot. We therefore show
that our implementation offers substantial start time improvements over even
the best case scenario of cold-booting.

\subsection{Benchmarks}

\begin{figure*}[t]
  \center
  \pgfplotstableread{forktime.tab}{\forktimetable}
  \pgfplotstableread{boottime.tab}{\boottimetable}

  \begin{tikzpicture}
  \begin{axis}[
      width=0.45\textwidth,
      title={Time for Virtual Machine Fork},
      xlabel={Guest Memory (MB)},
      ylabel={Time (ms)},
      xmin=0, xmax=2304,
      ymin=0, ymax=24,
      xtick={0, 512, 1024, 1536, 2048},
      ytick={0, 4, 8, 12, 16, 20, 24},
      ymajorgrids=true,
      xmajorgrids=true,
      grid style=dashed,
  ]

  \addplot+[
      color=blue,
      mark=square,
      only marks,
      error bars/.cd,
      y dir=both, y explicit
      ]
      table [
        y error minus = ly,
        y error plus = hy
      ] {\forktimetable};
  \end{axis}
  \end{tikzpicture}
  \label{fig:fork-time}
  \begin{tikzpicture}
  \begin{axis}[
      width=0.45\textwidth,
      title={Time for Virtual Machine Cold Start},
      xlabel={Guest Memory (MB)},
      ylabel={Time (ms)},
      xmin=0, xmax=2304,
      ymin=0, ymax=600,
      xtick={0, 512, 1024, 1536, 2048},
      ytick={0, 100, 200, 300, 400, 500, 600},
      ymajorgrids=true,
      xmajorgrids=true,
      grid style=dashed,
  ]

  \addplot+[
      color=blue,
      mark=square,
      only marks,
      error bars/.cd,
      y dir=both, y explicit
      ]
      table [
        y error minus = ly,
        y error plus = hy
      ] {\boottimetable};
  \end{axis}
  \end{tikzpicture}
  \caption{Time for virtual machine cold start vs. fork}
  \label{fig:boot-time}
\end{figure*}

% TODO: for each of these, we should justify why the benchmark is necessary
\Paragraph{Fork Time}
We run several experiments to test the time for a VM to fork. These tests start
a virtual machine, allocate some memory, and then fork.  The forking process
marks a timestamp when its fork call returns, and the child marks a timestamp
once it finishes restoring KVM state.  Figure~\ref{fig:fork-time} shows the
fork time as recorded by the child VM for varying amounts of allocated guest
memory.

Even though memory is not copied to the child process unless it is written, it
still takes time to walk through the parent page table and mark shared memory
as read-only, and generate page tables for the child process. For this reason
we expect to see fork times scale roughly linearly with the amount of memory
allocated pre-fork. Allocating more memory simulates a VM with more programs
and libraries loaded.

\begin{figure*}[t]

\center

\pgfplotstableread{pagefault.tab}{\cowtable}
\pgfplotstableread{nopagefault.tab}{\memtable}

\begin{tikzpicture}
\begin{axis}[
  width=0.45\textwidth,
  title={Time for Memory Pass, With Page Fault},
	xtick=data,
  symbolic x coords={128,256,512,1024,1536},
	ylabel={Time (ms)},
  xlabel={Pages Touched (MB)},
  legend style={at={(0.05,0.95)},anchor=north west},
	ybar,
	bar width=7pt,
  ymajorgrids=true,
  grid style=dashed
]
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = nofork,
  y error minus = noforklo,
  y error plus = noforkhi
  ] {\cowtable};
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = yesfork,
  y error minus = yesforklo,
  y error plus = yesforkhi
  ] {\cowtable};
  \legend{Before fork, After fork}
\end{axis}
\end{tikzpicture}
%\hspace{0.75in}
\begin{tikzpicture}
\begin{axis}[
  width=0.45\textwidth,
  title={Time for Memory Pass, No Page Fault},
	xtick=data,
  symbolic x coords={128,256,512,1024,1536},
	ylabel={Time (ms)},
  xlabel={Pages Touched (MB)},
  legend style={at={(0.05,0.95)},anchor=north west},
	ybar,
	bar width=7pt,
  ymajorgrids=true,
  grid style=dashed
]
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = nofork,
  y error minus = noforklo,
  y error plus = noforkhi
  ] {\memtable};
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = yesfork,
  y error minus = yesforklo,
  y error plus = yesforkhi
  ] {\memtable};
  \legend{Before fork, After fork}
\end{axis}
\end{tikzpicture}
\caption{Memory benchmark with and without copy-on-write ($n=100$)}
\label{fig:rambench}
\end{figure*}

\pgfplotstableread[col sep=comma,trim cells=true]{pybench.tab}{\pytable}
\pgfplotstableread[col sep=comma,trim cells=true]{pybenchtimes.tab}{\pytimetable}

\pgfplotsset{compat=1.5}
\begin{figure*}[t]
  \center
  \begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[
        width=0.45\textwidth,
      title={Python Benchmark Cumulative CPU Usage},
      xmin=0,xmax=240,
      ytick=data,
      symbolic y coords={None, After Boot, After Interpreter, After Packages},
      ylabel={Fork Point},
      xlabel={Cumulative CPU Time (s)},
      xbar,
      %bar width=7pt,
      xmajorgrids=true,
      grid style=dashed
    ]
      \addplot+[
        %error bars/.cd,
      %x dir=both, x explicit
    ]
      table [
        y = mode,
      x = total,
      %x error = moe,
    ] {\pytable};
    \end{axis}
  \end{tikzpicture}
  \label{fig:pybench}
\caption{Cumulative CPU time for 64 runs of python benchmark}
\end{figure*}

\begin{figure*}
  \center
  \begin{tabular}{|c|c|c|}
    \hline
    Fork Point & Benchmark Time (ms) & Relative Time \\ \hline
    None & $2500 \pm 9.14$ & $100$\% \\ \hline
    After Boot & $2433 \pm 8.36$ & $97.3$\% \\ \hline
    After Interpreter & $2409 \pm 8.36$ & $96.4$\% \\ \hline
    After Packages & $2401 \pm 8.04$ & $96.0$\% \\ \hline
  \end{tabular}

  \caption{Mean benchmark completion time (with $95$\% confidence interval)}
\end{figure*}

\Paragraph{Copy-on-Write Benchmark}
Though cloning VMs using Fork can decrease VM start times by orders of
magnitude because of shared memory, performing Copy-on-Write on those shared
regions has the potential to reduce performance. We implement a benchmark to
isolate the performance degradation caused by Copy-on-Write operations after
cloning.

This program begins by allocating $p$ pages of memory, and writes $b$ bytes of
random data to each one. It then signals to the host, which forks the VM $n$
times. In each child the program then writes more random data to each of those
pages.

Each of those post-fork writes will trap into the host kernel and induce a
copy. We compare the average time to completion of child VMs to the time it
takes a VM that has not forked to perform the same writes.

% TODO: we need to make this specific to the benchmark we actually ran
\Paragraph{Python Function}
In the lifecycle of a typical Function-as-a-Service unit, a VM is started, code
and data are copied on, and the user function is run. These services mostly use
runtimes like NodeJS or Python. To simulate this sort of workload, we use
pillow-perf, a Python image processing benchmark. This captures the typical
scenario of a function which loads an interpreter, loads some external
libraries, and performs some CPU-bound computation.

We compare the total time of starting $n$ separate VMs, and letting each run
the benchmark to completion, with the total time of forking $n$ VMs from one
reference image and letting them run to completion. [TODO: fork at different
times] The CPU performance of forked VMs should not differ from separately
booted VMs. Using a CPU-bound benchmark like pillow-perf verifies this
assumption.

\subsection{Results}

\Paragraph{Fork Time}
% TODO: write interpretation

\Paragraph{Copy-on-Write Benchmark}
Figure~\ref{fig:rambench} shows the results of our copy-on-write test. The two
graphs each show the stages of the benchmark: The blue columns show the time to
write random data to each page of a large memory allocation. The red columns
show the time to rewrite those pages. On the left, we fork between running
these two stages, so any memory accesses in the second stage will require a
page fault, copy, and resume. On the right we do not fork, so no page fault is
required. This experiment shows approximately a 28\% performance penalty for
memory accesses which trigger copy-on-write behavior.

\Paragraph{Python Function}
% TODO: write interpretation

