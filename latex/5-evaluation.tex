% !TEX root = top.tex above command is so that compilation is always from
% top.tex
\section{Evaluation} \label{sec:evaluation} We now describe the
design of our benchmarks for the kvmtool HyperFork implementation. We have
conducted both microbenchmarks, to test the overhead of fork and copy-on-write
page duplication, and end-to-end benchmarks to measure throughput and resource
utilization improvements.

\Paragraph{Experimental Setup}
We perform all tests on an m5.metal instance from Amazon Web Services EC2,
which features 96 logical processors and 384 GB of memory. The file system
image used for all tests is a minimal Linux setup generated using the buildroot
utility~\cite{buildroot}. This specially generated image contains only
the minimal set of utilities and software needed for our benchmarks. In typical
use, the guest images used for FaaS platforms may be more fully featured and
optimized, but such single purpose images have no need for the flexibility of a
general purpose Linux distribution image. Since FaaS uses very short-lived
instances, there is no need for package managers, update utilities, or other
maintenance software.

Using a minimal image also presents cold-boot times in an idealistic
environment, as a heavier image would take longer to boot. We therefore show
that our implementation offers substantial start time improvements over even
best case cold-boot scenarios.

\subsection{Benchmarks}

\Paragraph{Fork Time} As HyperFork replaces the time taken to boot a virtual
machine with the time taken to fork a virtual machine, we run several
experiments to test the time for a VM to fork. These tests start a virtual
machine, run basic userspace initialization, and then fork. The child VMM marks
a timestamp once it finishes restoring KVM state.

As memory is duplicated through copy-on-write in a Linux fork, memory is not
physically copied to the child process unitl it is modified. However, it still
takes time to walk through the parent page table and mark shared memory as
read-only and to generate page tables for the child process. For this reason we
expect to see fork times increase with the amount of memory allocated to the
virtual machine. We compare the results of this fork time benchmark with the
time to cold-boot a VM.

\Paragraph{Copy-on-Write} Though cloning VMs using fork can decrease VM start
times considerably, the guest memory mapped in the parent and child virtual
machine share physical pages until modified. As a result, modifications to
memory in each of the virtual machines can trigger page faults and memory
copies that may degrade performance. We implement a benchmark to isolate the
performance degradation caused by copy-on-write operations after cloning.

This program begins by allocating some amount of memory, and writes 128 bytes
of random data to each 4096-byte page allocated. This forces pages to be
allocated by the host kernel. It then signals to the host, which forks the VM.
In each child the program then performs an additional pass, again writing
random data to each of those pages.

Each of those post-fork writes will trap into the host kernel and induce a
copy. To assess copy-on-write overhead, we measure the time for each of these
memory passes. We compare passes that trigger a page fault (either due to
allocation pre-fork, or copying post-fork). We also compare passes that do not
trigger a page fault (both pre-fork and post-fork).

\Paragraph{Python Function} In the lifecycle of a typical FaaS request, a VM is
started, any relevant state is loaded, and the user-supplied function is run.
These services mostly use managed runtimes like NodeJS and Python. To simulate
a small latency-sensitive workload, we use the numpy FFT correctness test,
which runs in approximately $2.5$ seconds. This represents a somewhat long
latency-sensitive workload, and captures the typical scenario of a function
which starts an interpreter, loads external libraries, and performs a CPU-bound
computation.

We compare the total time of starting 64 separate VMs, and letting each run the
benchmark to completion, with the total time of forking an equal number of VMs
from one reference image and letting them run to completion. We consider the
total CPU time savings of starting each of these VMs by forking instead of
booting. We also consider a variety of different fork points, including
immediately after boot, after python loads, and after packages are loaded. We
also verify that benchmark performance is not degraded by the forking process.

\subsection{Results}

\begin{figure*}
  \center
  \pgfplotstableread{forktime.tab}{\forktimetable}
  \pgfplotstableread{boottime.tab}{\boottimetable}

  \begin{tikzpicture}
  \begin{axis}[
      width=0.45\textwidth,
      title={Time for Virtual Machine Fork},
      xlabel={Guest Memory (MB)},
      ylabel={Time (ms)},
      xmin=0, xmax=2304,
      ymin=0, ymax=24,
      xtick={0, 512, 1024, 1536, 2048},
      ytick={0, 4, 8, 12, 16, 20, 24},
      ymajorgrids=true,
      xmajorgrids=true,
      grid style=dashed,
  ]

  \addplot+[
      color=blue,
      mark=square,
      only marks,
      error bars/.cd,
      y dir=both, y explicit
      ]
      table [
        y error minus = ly,
        y error plus = hy
      ] {\forktimetable};
  \end{axis}
  \end{tikzpicture}
  \label{fig:fork-time}
  \hspace{1cm}
  \begin{tikzpicture}
  \begin{axis}[
      width=0.45\textwidth,
      title={Time for Virtual Machine Cold Start},
      xlabel={Guest Memory (MB)},
      ylabel={Time (ms)},
      xmin=0, xmax=2304,
      ymin=0, ymax=600,
      xtick={0, 512, 1024, 1536, 2048},
      ytick={0, 100, 200, 300, 400, 500, 600},
      ymajorgrids=true,
      xmajorgrids=true,
      grid style=dashed,
  ]

  \addplot+[
      color=blue,
      mark=square,
      only marks,
      error bars/.cd,
      y dir=both, y explicit
      ]
      table [
        y error minus = ly,
        y error plus = hy
      ] {\boottimetable};
  \end{axis}
  \end{tikzpicture}
  \caption{Time for virtual machine cold start vs. fork}
  \label{fig:boot-time}
\end{figure*}

\Paragraph{Fork Time} Figure~\ref{fig:boot-time} shows our measurement results
for our fork and boot time benchmarks. Error bars indicate the $2.5$- and
$97.5$-percentiles. Using the small image constructed for our benchmarks, we
observed boot times of approximately $563$ ms for a $512$ MB virtual machine.
Boot times increased slightly as memory size increased and overall exhibited
very low variance.

The fork operation, on the other hand, took an average of $7.43$ ms to complete
a fork of a $512$ MB virtual machine. This is an improvement of nearly two
orders of magnitude. Again, we observed very little variance in our
measurements. Fork time appears to scale linearly with the memory size of the
virtual machine, likely because the number of page mappings to be duplicated
scales linearly with the memory size. However, even at large memory sizes,
forking eliminates more than $95$\% of the boot time.

\begin{figure*}

\center

\pgfplotstableread{pagefault.tab}{\cowtable}
\pgfplotstableread{nopagefault.tab}{\memtable}

\begin{tikzpicture}
\begin{axis}[
  width=0.45\textwidth,
  title={Time for Memory Pass, No Page Fault},
	xtick=data,
  symbolic x coords={128,256,512,1024,1536},
	ylabel={Time (ms)},
  xlabel={Pages Touched (MB)},
  legend style={at={(0.05,0.95)},anchor=north west},
	ybar,
	bar width=7pt,
  ymajorgrids=true,
  grid style=dashed
]
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = nofork,
  y error minus = noforklo,
  y error plus = noforkhi
  ] {\memtable};
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = yesfork,
  y error minus = yesforklo,
  y error plus = yesforkhi
  ] {\memtable};
  \legend{Before fork, After fork}
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
  width=0.45\textwidth,
  title={Time for Memory Pass, With Page Fault},
	xtick=data,
  symbolic x coords={128,256,512,1024,1536},
	ylabel={Time (ms)},
  xlabel={Pages Touched (MB)},
  legend style={at={(0.05,0.95)},anchor=north west},
	ybar,
	bar width=7pt,
  ymajorgrids=true,
  grid style=dashed
]
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = nofork,
  y error minus = noforklo,
  y error plus = noforkhi
  ] {\cowtable};
\addplot+[
  error bars/.cd,
  y dir=both, y explicit
  ]
  table [
  x = mem,
  y = yesfork,
  y error minus = yesforklo,
  y error plus = yesforkhi
  ] {\cowtable};
  \legend{Before fork, After fork}
\end{axis}
\end{tikzpicture}
\caption{Memory benchmark with and without copy-on-write ($n=100$)}
\label{fig:rambench}
\end{figure*}

\pgfplotstableread[col sep=comma,trim cells=true]{pybench.tab}{\pytable}
\pgfplotstableread[col sep=comma,trim cells=true]{pybenchtimes.tab}{\pytimetable}

\pgfplotsset{compat=1.5}
\begin{figure*}
  \center
  \begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[
        width=0.45\textwidth,
      title={Python Benchmark Cumulative CPU Usage},
      xmin=0,xmax=240,
      ytick=data,
      symbolic y coords={None, After Boot, After Interpreter, After Packages},
      ylabel={Fork Point},
      xlabel={Cumulative CPU Time (s)},
      xbar,
      %bar width=7pt,
      xmajorgrids=true,
      grid style=dashed
    ]
      \addplot+[
        %error bars/.cd,
      %x dir=both, x explicit
    ]
      table [
        y = mode,
      x = total,
      %x error = moe,
    ] {\pytable};
    \end{axis}
  \end{tikzpicture}
  \label{fig:pybench}
\caption{Cumulative CPU time for 64 runs of python benchmark}
\label{fig:pycpu}
\end{figure*}

\begin{figure*}
  \center
  \begin{tabular}{|c|c|c|}
    \hline
    Fork Point & Benchmark Time (ms) & Relative Time \\ \hline
    None & $2500 \pm 9.14$ & $100$\% \\ \hline
    After Boot & $2433 \pm 8.36$ & $97.3$\% \\ \hline
    After Interpreter & $2409 \pm 8.36$ & $96.4$\% \\ \hline
    After Packages & $2401 \pm 8.04$ & $96.0$\% \\ \hline
  \end{tabular}

  \caption{Mean benchmark completion time (with $95$\% confidence interval)}
  \label{fig:pybench}
\end{figure*}


\Paragraph{Copy-on-Write} Figure~\ref{fig:rambench} shows the results
of our copy-on-write test. Each graph shows the performance before and after
the fork operation. Error bars indicate $2.5$- and $97.5$-percentiles. When
memory writes do not trigger a page fault (left plot), performance does not
appear to change before or after a page fault. This means that after pages have
been allocated (or reallocated after being copied on write), there is no
significant memory performance penalty.

When writes do trigger a page fault (right plot), we observe a considerable
difference before and after the fork operation. Before the fork operation, page
faults occur when we write to pages that have not yet been allocated. The
kernel then allocates a zeroed page and returns. After the fork operation, page
faults trigger a page copy. We observe a roughly $28\%$ performance penalty for
the memory benchmark when pages must be copied. As expected, the benchmark time
scales roughly linearly with the number of pages for which a page fault is
triggered.

\Paragraph{Python Function} Figure~\ref{fig:pycpu} displays the cumulative CPU
time required for the python benchmark. The margins of error are negligible
(less than $0.2$\%). We see a significant improvement when forking compared to
booting 64 independent virtual machines. Forking after boot immediately results
in a $24.8$\% CPU time savings across the 64 benchmarks. Forking after starting
the python interpreter saves $26.7$\% over not forking. Forking after packages
are loaded saves $30.2\%$ over not forking. Thus forking instead of booting new
virtual machines can result in significant resource savings for servers running
short duration or latency sensitive serverless workloads. This effect would be
accentuated for shorter jobs and more minor for longer ones.

Furthermore, Figure~\ref{fig:pybench} shows the mean benchmark completion times
for each forking scheme. Interestingly, we do not observe any performance
degradation when forking instead of booting. In fact, forking exhibits a slight
(but statistically significant) reduction in benchmark completion time. We
suspect this may be due to increased memory locality due to shared pages after
forking.
